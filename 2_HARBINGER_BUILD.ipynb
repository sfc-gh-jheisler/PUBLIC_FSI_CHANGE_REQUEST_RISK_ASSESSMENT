{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "y2mfq24pyvjtb7623o3f",
   "authorId": "217400574159",
   "authorName": "JHEISLER",
   "authorEmail": "john.heisler@snowflake.com",
   "sessionId": "c1774b2f-51d5-4df0-843b-c6d996d3c7d3",
   "lastEditTime": 1738856553779
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530ddf9c-b439-440a-8a64-355acf3ed172",
   "metadata": {
    "name": "md_synopsis",
    "collapsed": false,
    "resultHeight": 2228
   },
   "source": "# Change Request Risk Assessment\n#### Author: **John Heisler** - Senior AI Specialist, Financial Services, Snowflake\nIn this notebook, we're going to evaluate a new change request and determine a risk score for that change request resulting in target environment instability.\n\nThe notebook is written to allow deployment directly into anyone's environment.\n\n## Hit the Nail, Don't Build the Hammer!\nUltimately, AI should drive commercial value. To that end, building, deploying, and maintaining the systems that underpin that commercial value need to be easy, efficient, and trusted. \nEnterprises that maximize efforts leveraging their differentiated domain knowledge and creativity will ultimately win the AI race. Every ounce of effort spent on managing/building/tuning AI is a distraction from delivering value and thus comes at a material opportunity cost. With Snowflake Cortex AI, we aim to maximize efforts wielding the power of AI, not building it.\n\n## Use Case Commercial Value\nOur solution drives **operational alpha** by maximizing uptime of production environments. This solution provides operational alpha in at least these four ways:\n1. **System Uptime**: The systems critical to support our portfolio teams will be more stable and provide maximum value to our portfolio teams' performance.\n2. **Regulatory Reporting**: Minimize need for reporting to external regulators about critical system downtime and its impact.\n3. **Opportunity Cost**: Minimizing time spent conducting emergency maintenance which can be repurposed to focus on alpha-generating solutions.\n4. **Opportunity Cost**: Minimize need for time-and-resource-intensive root cause analysis and these cross functional teams can focus on their primary responsibilities.\n\n## Process Outline\nBefore we got here, using the harbinger_data_creation.ipynb notebook in this repo, we created and filled a change request table with some synthetic data (Fun Fact: You we use LLMs to do that-- check out the aforementioned notebook to see how).\n\n1. Build a python function to house our prompt and accept a change request as context. this approach streamlines our code and decouples the prompt from our broader development, allowing for independent development on the prompt by domain experts.\n2. Build a complete function to streamline passing of parameters to our complete function.\n3. Generate Risk Scores and Reasonings with batch inference whereby we generate a risk score for change requests as they appear in our system.\n4. Build a UI to allow the end user to interact with the AI.   \n    * Summary Interface: leveraging the data that we persisted from our batch inference in step 3, render the results for review. This portion of the infercae is great for weekly reviews of change requests\n    * Ad Hoc/Exploratory Interface: Allow an end user to pass a Change Request to an LLM of their choosing to explore how different LLMs perform on this task.\n\n\n## Solution Differentiators\n* **LLM fungibility**: We have a model garden right here in Snowflake-- no need to manage divergent infrastructure and no need for external calls which introduce risk into your system\n* **Obeying the Laws of Data Physics**: We develop, perform inference, and host the UI in the same compute environment that the data resides-- eleminating bloated architectures and complexity associated with managing the data, inference, and UI in seperate infrastructures. \n\n## Art of the Possible\nHere is some food for thought and hopefully inspiring enhancements for your deployment. \n\n* **Fine Tuning**: If we have tied incidents' root causes to change requests in the past, we could fine tune a model here in Snowflake with that data and the task that we're after. This has a couple of very interesting advantages. first, we would have a very specialize model that may perform this task very well because it has \"experience\" with what good and bad looks like. More operationally focused, we could maybe use a smaller model maximizing cost efficiencies.\n* **More Context - Metadata**: we could offer much more context about what each column in the data set means and fully define the json structure that we're passing. This would inform the model on the meaning of each column rather than allowing it to come up with its best guess at the columns.\n* **More Context - Target System Stability (Windshield)**: We could use another LLM upstream of the final risk inference to build a synopsis of the target system stability and status. For instance, it could state things like, \"the target system for EDM-Account-Master has seen several out of memory alerts and disc space errors in the last six weeks.\" We would instruct the risk prompt to consider that environment when deriving its risk assessment which should enhance its predictions.\n* **More Context - Incident Root Causes (Rear view Mirror)**: We could pass a synopsis of the last 6 months of root causes to the model. This would make it keenly aware \\"
  },
  {
   "cell_type": "markdown",
   "id": "2495c655-8b74-4188-87ab-4d357608b068",
   "metadata": {
    "name": "md_prompt_fucntion",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# Create Prompt Function with Python"
  },
  {
   "cell_type": "code",
   "id": "59beacfd-a764-489e-ab93-80fc1e621c05",
   "metadata": {
    "language": "python",
    "name": "py_generate_risk_prediction_prompt",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "def generate_risk_prediction_prompt(cr_data):\n    prompt = f\"\"\"\n            <role>\n                You are an experienced dev ops professional deeply knowledgeable on computer systems that support a very large company and the metadata that is captured about change requests.\n                A change request is a formal proposal for an alteration to the computer system that you manage.\n                As a dev ops expert, you specialize in using the metadata provided about a change request to predict the liklihood of the change request unintentionally destabalizing the computer system.\n                You are going to be provided with change request meta data as a json object held between <cr_data> and your job is to provide a prediction score and reasoning behind the risk score in the <output> section. \n            </role>\n        \n            <task>: Follow these instructions,\n                1) Considering the <cr_data> and your <role>, provide a risk score between 0 to 5 of this change request destabalizing the computer system when deployed. do not exhibit a bias toward high risk. base your risk score only on the data you have been provided. if there is not enough information, please indicate this. Output this as [Risk_Score]. Then,\n                2) Considering the <cr_data> and your <role>, provide a reasoning for the risk score in as few words as possible while maintaining all detail needed to understand your reasoning. Output this as [Risk_Score_Reason]\n            </task>\n\n            <meta_data>\n                The folowing is the metadata for the cr_data. the metatdata follows this format: (column_name: data_type : description : sample_values):\n                \n                (Description: VARCHAR(16777216) : A summary of the change : ''Upgrade to the existing DataSync API from version 3.2 to 3.4 in the Production environment. The update includes several key performance optimizations, enhanced security features, and bug fixes that address issues with data consistency and processing time.\n                The major components of this change include:\n                \n                API Version Update: Migrating from DataSync API v3.2 to v3.4 to support faster data ingestion and processing.\n                Security Enhancements: Implementation of OAuth 2.0-based authentication to replace the legacy basic authentication mechanism, improving overall security for API transactions.\n                Error Handling: Enhanced error codes and more descriptive responses for improved debugging in the event of failure.\n                Database Schema Update: Modifications to the backend MySQL database to accommodate new data types introduced in version 3.4.\n                Testing will be performed in the staging environment (v3.4-Stage) before deployment to ensure backwards compatibility with existing systems. No downtime is expected during the deployment, but a rollback plan has been prepared in case of critical issues.'',)\n                   \n                (Date: DATE : The date the request was made : 2025-02-03)\n                \n                (Impact: VARCHAR(4000) : How the change will affect the project, including cost, quality, risk, scope, duration, and schedule)\n                \n                (Priority: VARCHAR(400) : How quickly the change should be approved and implemented : ''immediately'', ''within release window'', ''after approval'')\n                \n                (Risk: VARCHAR(400) : The risk level of the change as described by development team : ''low risk'', ''moderate risk'', ''high risk'')\n                \n                (Justification: VARCHAR(4000) : The reason for the change : ''preventative maintenance'', ''patch'', ''planned release as part of project koala'')\n                \n                (State: VARCHAR(400) : The status of the change request : ''new'', ''under review'', ''approved'', ''deferred'', ''rejected'')\n                \n                (Disposition: VARCHAR(400) : An explanation for approved, deferred, or rejected changes : ''no peer review'', ''manager override'', ''uniform agreement'')\n                \n                (Category: VARCHAR(400) : The category of the change : ''planned'', ''unplanned'', ''emergency'')\n                \n                (Change number: VARCHAR(100) : A unique ID for tracking the request :''AB1672'', ''723CS'', ''6D62EE'')\n            </meta_data>\n            \n            <cr_data>\n                {cr_data}\n            </cr_data>\n        \n            <Output> \n                produce valid JSON. Absolutely do not include any additional text before or following the JSON. Output should use following <JSON_format>\n            </Output>\n            \n            <JSON_format>\n            {{\n                \"Risk_Score\": (A risk score between 0 to 5 of this change request destabalizing the computer system when deployed),\n                \"Risk_Score_Reason\": (A concise resoning for the Risk_Score and any suggestions to mitigate),\n            }}\n            </JSON_format>\n            \"\"\"\n    return prompt",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c93165dc-fae8-4d98-a81c-6450b4981625",
   "metadata": {
    "name": "md_my_complete",
    "collapsed": false
   },
   "source": "# my_complete() \n\nWe can call our LLMs via the LLM API (https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-llm-rest-api) or using SQL. To make passing our parameters easy, I am coing to use SQL and wrap it in a python function to call later."
  },
  {
   "cell_type": "code",
   "id": "5acf80bb-8c0c-41a6-a711-be778dc7b1e8",
   "metadata": {
    "language": "python",
    "name": "py_create_complete_function",
    "collapsed": false
   },
   "outputs": [],
   "source": "def my_complete(model, context, temp = 0, max_tokens: int = 18000):\n    sql = F\"\"\"SELECT SNOWFLAKE.CORTEX.COMPLETE(\n            '{model}',\n            [\n                {{\n                    'role': 'user',\n                    'content': '{context}'\n                }}\n            ],\n            {{\n                'max_tokens': {max_tokens}, \n                'temperature' : {temp}\n            }}\n        ) as inference;\"\"\"\n    inference_raw = session.sql(sql).to_pandas().loc[0,\"INFERENCE\"]\n    inference_json = json.loads(inference_raw)\n    inference_raw = inference_json['choices'][0]['messages']\n    return inference_raw",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "303c090f-351a-46a4-8ccb-e94759ce1106",
   "metadata": {
    "name": "md_risk_score",
    "collapsed": false,
    "resultHeight": 202
   },
   "source": "# Batch Inference\n## Generate Risk Score and Reasoning\n\n\n\n### ü§Ø Whoa, check that out ü§Ø\nIn a single line of python (10), we compile our prompt and call our LLM for inference. We could swap ANY LLM WE WANT here with exactly 0 overhead. \n\nhttps://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#availability"
  },
  {
   "cell_type": "code",
   "id": "25ddc7d2-4ba7-4e45-9a01-0c38ada9a3a3",
   "metadata": {
    "language": "python",
    "name": "py_batch_inference"
   },
   "outputs": [],
   "source": "import streamlit as st\n\nsession = get_active_session()\ndatabase = 'GEN_AI_FSI'\nschema = 'DTCC_HACKATHON'\ntable = 'CHANGE_REQUEST_RAW'\n\ndf = session.table(f\"{database}.{schema}.{table}\").to_pandas()\n\ndf['RISK_ASSESSMENT'] = df.apply(lambda row: my_complete('claude-3-5-sonnet', generate_risk_prediction_prompt(row.to_json())), axis = 1)\n\n#caution-- this will generate inference for each row in teh dataset and may tame a moment to run. \n#You can also use df.head() to run this on a sample\nsession.write_pandas(df, table_name=\"CHANGE_REQUEST_RISK_INFERENCE\", database=\"GEN_AI_FSI\", schema=\"DTCC_HACKATHON\", auto_create_table=True, overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3e81d7d6-905f-4044-ad33-86d2090c4cb6",
   "metadata": {
    "name": "md_SIS",
    "collapsed": false,
    "resultHeight": 115
   },
   "source": "# Make It Relevant with Streamlit\n\nBuilt but not accessible is the same as not built at all, we need to expose this functionlity to the end user. We will use Streamlit in Snowflake to do that. \n\nWe want to give the end user the abilility to interact with this data in two ways: \n1. First we want them to be able to see all of their change requests in a single place-- this would be great for a systematic or weekly review of changes. \n2. Give the end users a way to select a change request and use an LLM of their choosing to generate the risk score and reasoning.\n\n**Note**: A combined and polished version of this can be found in 3_HARBINGER_UI.py in this repository."
  },
  {
   "cell_type": "code",
   "id": "6671ee68-a4c5-4f0b-b695-29fe8a516d58",
   "metadata": {
    "language": "python",
    "name": "py_summary_streamlit",
    "collapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st\nimport json5 as json\nimport pandas as pd\nfrom snowflake.snowpark.functions import col, call_udf\nfrom snowflake.snowpark.context import get_active_session\n\n# # Page configuration\n# st.set_page_config(\n#     page_title=\"Change Request Risk Assessment\",\n#     page_icon=\"üîç\",\n#     layout=\"wide\",\n#     initial_sidebar_state=\"expanded\"\n# )\n\n# Add legend for risk score colors\nst.sidebar.title('Risk Score Legend')\n\nfunctionality = st.sidebar.selectbox('Select Functionlity', (\"Summary\", \"Ad Hoc\"))\n\nlegend_cols = st.sidebar.columns(3)\nwith legend_cols[0]:\n    st.color_picker('Low Risk (0-2)', '#00FF00', disabled=True)\nwith legend_cols[1]:\n    st.color_picker('Medium Risk (2-3)', '#FFFF00', disabled=True)\nwith legend_cols[2]:\n    st.color_picker('High Risk (4-5)', '#FF0000', disabled=True)\n\n# Custom CSS for elegant styling\nst.write(\"\"\"\n<style>\n    .stButton button {\n        background-color: #4CAF50;\n        color: white;\n        padding: 0.5rem 1rem;\n        border-radius: 5px;\n        border: none;\n        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n        transition: all 0.3s ease;\n    }\n    .stButton button:hover {\n        background-color: #45a049;\n        box-shadow: 0 4px 8px rgba(0,0,0,0.2);\n    }\n    div[data-testid=\"stDecoration\"] {\n        background-image: none;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n# Initialize session and get data\nsession = get_active_session()\ndatabase = 'GEN_AI_FSI'\nschema = 'DTCC_HACKATHON'\ntable = 'CHANGE_REQUEST_RISK_INFERENCE'\n\ndef color_risk_score(val):\n    \"\"\"\n    Returns a color based on the risk score value:\n    - Low risk (0-1): Green\n    - Medium risk (2-3): Yellow\n    - High risk (4-5): Red\n    \"\"\"\n    if pd.isna(val):\n        return ''\n    \n    # Normalize value between 0 and 1\n    normalized = float(val) / 10\n    \n    # Create RGB values for gradient\n    if normalized < 0.2:  # Green to Yellow\n        r = int(255 * (normalized / 0.2))\n        g = 255\n        b = 0\n    else:  # Yellow to Red\n        r = 255\n        g = int(255 * (1 - (normalized - 0.2) / 0.5))\n        b = 0\n    \n    return f'background-color: rgba({r}, {g}, {b}, 0.2)'\n\n@st.cache_data\ndef load_data():\n    df = session.table(f\"{database}.{schema}.{table}\").to_pandas()\n    for i in range(len(df)):\n        json_data = json.loads(df.at[i, 'RISK_ASSESSMENT'])\n        df.at[i, 'RISK_SCORE_AI'] = json_data[\"Risk_Score\"]\n        df.at[i, 'RISK_REASON_AI'] = json_data[\"Risk_Score_Reason\"]\n    return df\n\n# Load data\ndf = load_data()\n\n# Prepare display tables with styling\ndisplay_table = df[['CHANGENUMBER', 'DATE', 'RISK_SCORE_AI', 'RISK_REASON_AI']]\ndisplay_table_detail = df[['CHANGENUMBER', 'DESCRIPTION', 'DATE', 'IMPACT', \n                          'PRIORITY', 'RISK', 'JUSTIFICATION', 'STATE', \n                          'DISPOSITION', 'CATEGORY', 'RISK_SCORE_AI', 'RISK_REASON_AI']]\n\n# App header\nst.title('Risk Assessment Dashboard')\n\n# Add some basic metrics\nst.subheader('Key Metrics')\nmetric_col1, metric_col2, metric_col3 = st.columns(3)\n\nwith metric_col1:\n    avg_risk = df['RISK_SCORE_AI'].mean()\n    st.metric('Average Risk Score', f'{avg_risk:.2f}')\n\nwith metric_col2:\n    high_risk_count = len(df[df['RISK_SCORE_AI'] > 3])\n    st.metric('High Risk Changes', high_risk_count)\n\nwith metric_col3:\n    total_changes = len(df)\n    st.metric('Total Changes', total_changes)\n\n# Main content area\ncol1, col2 = st.columns([3, 1])\n\ntitle = 'Risk Assessment Summary'\n\nst.subheader(title)\nshow_details = st.button('Show Detailed View', use_container_width=True)\n    \n# Show detailed view if button is clicked\nif show_details:\n    title = 'Detailed Risk Assessment'\n    st.dataframe(\n        display_table_detail.style.applymap(\n            color_risk_score,\n            subset=['RISK_SCORE_AI']\n        ),\n        use_container_width=True,\n        hide_index=True,\n        height=400\n)\nelse: \n    st.dataframe(\n        display_table.style.applymap(\n            color_risk_score,\n            subset=['RISK_SCORE_AI']\n        ),\n        use_container_width=True,\n        hide_index=True,\n        height=400\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e541c9e-b51c-468a-95e3-999bda41df8c",
   "metadata": {
    "language": "python",
    "name": "py_full_application",
    "collapsed": false,
    "resultHeight": 733
   },
   "outputs": [],
   "source": "import streamlit as st\nimport json5 as json\nimport pandas as pd\nfrom snowflake.snowpark.functions import col, call_udf\nfrom snowflake.snowpark.context import get_active_session\n\n# Page configuration\n# st.set_page_config(\n#     page_title=\"Change Request Risk Assessment\",\n#     page_icon=\"üîç\",\n#     layout=\"wide\",\n#     initial_sidebar_state=\"expanded\"\n# )\n\n# Add legend for risk score colors\nst.sidebar.title('Risk Score Legend')\n\nfunctionality = st.sidebar.selectbox('Select Functionlity', (\"Summary\", \"Ad Hoc\"))\n\nlegend_cols = st.sidebar.columns(3)\nwith legend_cols[0]:\n    st.color_picker('Low Risk (0-2)', '#00FF00', disabled=True)\nwith legend_cols[1]:\n    st.color_picker('Medium Risk (2-3)', '#FFFF00', disabled=True)\nwith legend_cols[2]:\n    st.color_picker('High Risk (4-5)', '#FF0000', disabled=True)\n\n##########################\n##########AD HOC##########\n##########################\n\n# Custom CSS to enhance the UI\nst.markdown(\"\"\"\n    <style>\n    .risk-score {\n        font-size: 24px;\n        font-weight: bold;\n        padding: 1rem;\n        border-radius: 8px;\n        margin: 1rem 0;\n    }\n    .info-box {\n        background-color: #f8f9fa;\n        padding: 1.5rem;\n        border-radius: 8px;\n        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n    }\n    </style>\n    \"\"\", unsafe_allow_html=True)\n\n# Header with gradient background\nst.title(\"Change Request Risk Assessment\")\n\n# Initialize session\nsession = get_active_session()\n\ndef generate_risk_prediction_prompt(cr_data):\n    prompt = f\"\"\"\n            <role>\n            You are an experienced dev ops professional deeply knowledgeable on computer systems that support a very large company and the metadata that is captured about change requests.\n            A change request is a formal proposal for an alteration to the computer system that you manage.\n            As a dev ops expert, you specialize in using the metadata provided about a change request to predict the liklihood of the change request unintentionally destabalizing the computer system.\n            You are going to be provided with change request meta data as a json object held between <cr_data> and your job is to provide a prediction score and reasoning behind the risk score in the <output> section. \n            </role>\n        \n            <task>: Follow these instructions,\n            1) Considering the <cr_data> and your <role>, provide a risk score between 0 to 5 of this change request destabalizing the computer system when deployed. do not exhibit a bias toward high risk. base your risk score only on the data you have been provided. if there is not enough information, please indicate this. Output this as [Risk_Score]. Then,\n            2) Considering the <cr_data> and your <role>, provide a reasoning for the risk score in as few words as possible while maintaining all detail needed to understand your reasoning. Output this as [Risk_Score_Reason]\n            </task>\n\n            <cr_data>\n            {cr_data}\n            </cr_data>\n        \n            <Output> \n            produce valid JSON. Absolutely do not include any additional text before or following the JSON. Output should use following <JSON_format>\n            </Output>\n            \n            <JSON_format>\n            {{\n                \"Risk_Score\": (A risk score between 0 to 5 of this change request destabalizing the computer system when deployed),\n                \"Risk_Score_Reason\": (A concise resoning for the Risk_Score and any suggestions to mitigate),\n            }}\n            </JSON_format>\n            \"\"\"\n    return prompt\n\ndef my_complete(model, context, temp=0, max_tokens: int = 18000):\n    sql = F\"\"\"SELECT SNOWFLAKE.CORTEX.COMPLETE(\n            '{model}',\n            [\n                {{\n                    'role': 'user',\n                    'content': '{context}'\n                }}\n            ],\n            {{\n                'max_tokens': {max_tokens}, \n                'temperature' : {temp} \n            }}\n        ) as inference;\"\"\"\n    inference_raw = session.sql(sql).to_pandas().loc[0,\"INFERENCE\"]\n    inference_json = json.loads(inference_raw)\n    inference_raw = inference_json['choices'][0]['messages']\n    return inference_raw\n\n# Database configuration\ndatabase = 'GEN_AI_FSI'\nschema = 'DTCC_HACKATHON'\ntable = 'ChangeRequests'\nhistory_table = 'CHANGE_REQUEST_RISK_INFERENCE'\n\n# Get the data into pandas\ncr_df = session.table(f\"{database}.{schema}.{table}\")\n\n# Create two columns for the input section\ncol1, col2 = st.columns(2)\n\nwith col1:\n    #st.markdown('<div class=\"info-box\">', unsafe_allow_html=True)\n    cr_request = st.selectbox(\n        'Select a Change Request',\n        cr_df,\n        help=\"Choose the change request you want to analyze\"\n    )\n    st.markdown('</div>', unsafe_allow_html=True)\n\nwith col2:\n    #st.markdown('<div class=\"info-box\">', unsafe_allow_html=True)\n    # Available models\n    models = [\n        'snowflake-arctic', 'claude-3-5-sonnet', 'mistral-large',\n        'mistral-large2', 'reka-flash', 'reka-core', 'jamba-instruct',\n        'jamba-1.5-mini', 'jamba-1.5-large', 'mixtral-8x7b',\n        'llama2-70b-chat', 'llama3-8b', 'llama3-70b', 'llama3.1-8b',\n        'llama3.1-70b', 'llama3.3-70b', 'snowflake-llama-3.3-70b',\n        'llama3.1-405b', 'snowflake-llama-3.1-405b', 'llama3.2-1b',\n        'llama3.2-3b', 'mistral-7b', 'gemma-7b'\n    ]\n    \n    user_input_model = st.selectbox(\n        \"Select AI Model\",\n        models,\n        help=\"Choose the AI model for risk assessment\",\n        key=\"CS_model_select_box\"\n    )\n    st.markdown('</div>', unsafe_allow_html=True)\n\n# Add a loading spinner\nwith st.spinner(\"Analyzing risk...\"):\n    df = session.table(f\"{database}.{schema}.{table}\").filter(col(\"CHANGENUMBER\") == cr_request).to_pandas()\n    \n    if st.button(\"Analyze Risk\", type=\"primary\"):\n        try:\n            df['RISK_ASSESSMENT'] = df.apply(\n                lambda row: my_complete(user_input_model, generate_risk_prediction_prompt(row.to_json())),\n                axis=1\n            )\n            \n            json_data = json.loads(df.at[0, 'RISK_ASSESSMENT'])\n            \n            # Create three columns for the results\n            result_col1, result_col2, result_col3 = st.columns([1,2,1])\n            \n            with result_col1:\n                #st.markdown('<div class=\"info-box\">', unsafe_allow_html=True)\n                # Color code the risk score\n                risk_score = float(json_data[\"Risk_Score\"])\n                color = \"green\" if risk_score <= 2 else \"orange\" if risk_score <= 3.5 else \"red\"\n                st.markdown(f'<div class=\"risk-score\" style=\"background-color: {color}; color: white;\">'\n                          f'Risk Score: {risk_score}</div>', unsafe_allow_html=True)\n                st.markdown('</div>', unsafe_allow_html=True)\n            \n            with result_col2:\n                #st.markdown('<div class=\"info-box\">', unsafe_allow_html=True)\n                st.subheader(\"Risk Assessment\")\n                st.write(json_data[\"Risk_Score_Reason\"])\n                st.markdown('</div>', unsafe_allow_html=True)\n            \n            with result_col3:\n                with st.expander(\"View Raw JSON\"):\n                #if st.button(\"View Raw JSON\"):\n                    st.json(json_data)\n\n        except ValueError as e:\n            st.error(f\"Error: {user_input_model} did not produce valid output. Please select another model.\")\n            st.exception(e)\n        except Exception as e:\n            st.error(\"An unexpected error occurred. Please try again.\")\n            st.exception(e)\n",
   "execution_count": null
  }
 ]
}